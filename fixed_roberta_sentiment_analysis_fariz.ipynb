{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n"
      ],
      "metadata": {
        "id": "Mf_13Hn5ksj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv(\"PD_sentiment_analysis.csv\")"
      ],
      "metadata": {
        "id": "MEGDYhU1oLtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"w11wo/indonesian-roberta-base-sentiment-classifier\")\n",
        "\n",
        "# Tokenize\n",
        "tokenized_data = tokenizer(dataset[\"content\"].tolist(), return_tensors=\"tf\", padding=True)\n",
        "\n",
        "# Convert labels\n",
        "labels = np.array(dataset[\" label\"])  # Corrected column name"
      ],
      "metadata": {
        "id": "lQeJNnHtoGKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = 'w11wo/indonesian-roberta-base-sentiment-classifier'\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3)  # Assuming 3 classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQonIY4QoO9P",
        "outputId": "f2b0b1a1-4432-408e-bf78-e65eca388131"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
            "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "NkP0GtSkoUmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameter\n",
        "num_epochs = 15\n",
        "batch_size = 32\n",
        "validation_split = 0.2\n",
        "\n",
        "# Training\n",
        "history = model.fit(\n",
        "    tokenized_data,\n",
        "    labels,\n",
        "    epochs=num_epochs,\n",
        "    batch_size=batch_size,\n",
        "    validation_split=validation_split\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(tokenized_data, labels)\n",
        "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n",
        "\n",
        "# Save model\n",
        "model.save_pretrained(\"./fine_tuned_model\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKkoAbw-obG7",
        "outputId": "dcad261f-d266-4e0c-9caa-74d91a1c7b6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "1/1 [==============================] - 57s 57s/step - loss: 1.3294 - accuracy: 0.7000 - val_loss: 8.2001 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/15\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.0104 - accuracy: 0.6000 - val_loss: 1.3575 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/15\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.1997 - accuracy: 0.4000 - val_loss: 0.1314 - val_accuracy: 1.0000\n",
            "Epoch 4/15\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.0048 - accuracy: 0.1000 - val_loss: 6.8413 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/15\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.4148 - accuracy: 0.6000 - val_loss: 0.0396 - val_accuracy: 1.0000\n",
            "Epoch 6/15\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.4767 - accuracy: 0.1000 - val_loss: 9.1678 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/15\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.7158 - accuracy: 0.3000 - val_loss: 8.8887 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/15\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.6041 - accuracy: 0.6000 - val_loss: 8.5221 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/15\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.6087 - accuracy: 0.6000 - val_loss: 7.5426 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/15\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.2811 - accuracy: 0.6000 - val_loss: 6.6263 - val_accuracy: 0.0000e+00\n",
            "Epoch 11/15\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.3644 - accuracy: 0.2000 - val_loss: 5.6194 - val_accuracy: 0.0000e+00\n",
            "Epoch 12/15\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.3236 - accuracy: 0.3000 - val_loss: 4.4115 - val_accuracy: 0.0000e+00\n",
            "Epoch 13/15\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.0287 - accuracy: 0.6000 - val_loss: 3.2964 - val_accuracy: 0.0000e+00\n",
            "Epoch 14/15\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.9586 - accuracy: 0.6000 - val_loss: 2.2391 - val_accuracy: 0.0000e+00\n",
            "Epoch 15/15\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.0143 - accuracy: 0.6000 - val_loss: 1.3833 - val_accuracy: 0.0000e+00\n",
            "1/1 [==============================] - 1s 565ms/step - loss: 1.1312 - accuracy: 0.4615\n",
            "Test Loss: 1.1311838626861572, Test Accuracy: 0.4615384638309479\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./fine_tuned_model/tokenizer_config.json',\n",
              " './fine_tuned_model/special_tokens_map.json',\n",
              " './fine_tuned_model/vocab.json',\n",
              " './fine_tuned_model/merges.txt',\n",
              " './fine_tuned_model/added_tokens.json',\n",
              " './fine_tuned_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKvSAa_BkG1G",
        "outputId": "ded9fbb5-a7e5-4a71-a560-ecffac8aa1f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at ./fine_tuned_model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "    {\n",
            "        \"label\": \"positive\",\n",
            "        \"score\": \"0.251\"\n",
            "    },\n",
            "    {\n",
            "        \"label\": \"neutral\",\n",
            "        \"score\": \"0.118\"\n",
            "    },\n",
            "    {\n",
            "        \"label\": \"negative\",\n",
            "        \"score\": \"0.631\"\n",
            "    }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "# Load model\n",
        "fine_tuned_model_path = \"./fine_tuned_model\"\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(fine_tuned_model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(fine_tuned_model_path)\n",
        "\n",
        "# Predict text\n",
        "text = \"argumen yang bapak berikan kurang bagus\"\n",
        "\n",
        "# Tokenize\n",
        "inputs = tokenizer(text, return_tensors=\"tf\", padding=True, truncation=True)\n",
        "outputs = model(inputs)\n",
        "\n",
        "# Get the predicted class probabilities\n",
        "logits = outputs.logits\n",
        "probabilities = tf.nn.softmax(logits, axis=-1).numpy()[0]\n",
        "\n",
        "# Get the class labels from the config\n",
        "id2label = {0: \"positive\", 1: \"neutral\", 2: \"negative\"}\n",
        "\n",
        "# Prepare the output as a list of dictionaries\n",
        "output_list = [{\"label\": label, \"score\": f\"{prob:.3f}\"} for label, prob in zip(id2label.values(), probabilities)]\n",
        "\n",
        "# Convert the list to a JSON string\n",
        "output_json = json.dumps(output_list, indent=4)\n",
        "\n",
        "# Print the JSON string\n",
        "print(output_json)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the JSON string to a file in the fine_tuned_model directory\n",
        "file_path = f\"{fine_tuned_model_path}/sentiment_analysis_pred.json\"\n",
        "with open(file_path, \"w\") as json_file:\n",
        "    json_file.write(output_json)\n",
        "\n",
        "print(f\"Output saved to {file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fS5d-BGYk32P",
        "outputId": "8eee6246-699f-47a9-a64b-c75919cff28e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved to ./fine_tuned_model/sentiment_analysis_pred.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vC0m7u2ZoaPN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}